# 一些基本概念

在開始學習Keras之前，我們希望傳遞一些關於Keras，關於深度學習的基本概念和技術，我們建議新手在使用Keras之前瀏覽一下本頁面提到的內容，這將減少你學習中的困惑

## 符號計算

Keras的底層庫使用Theano或TensorFlow，這兩個庫也稱為Keras的後端。無論是Theano還是TensorFlow，都是一個“符號式”的庫。

因此，這也使得Keras的編程與傳統的Python代碼有所差別。籠統的說，符號主義的計算首先定義各種變量，然後建立一個“計算圖”，計算圖規定了各個變量之間的計算關係。建立好的計算圖需要編譯以確定其內部細節，然而，此時的計算圖還是一個“空殼子”，裡面沒有任何實際的數據，只有當你把需要運算的輸入放進去後，才能在整個模型中形成數據流，從而形成輸出值。

就像用管道搭建供水系統，當你在拼水管的時候，裡面是沒有水的。只有所有的管子都接完了，才能送水。

Keras的模型搭建形式就是這種方法，在你搭建Keras模型完畢後，你的模型就是一個空殼子，只有實際生成可調用的函數後（K.function），輸入數據，才會形成真正的數據流。

使用計算圖的語言，如Theano，以難以調試而聞名，當Keras的Debug進入Theano這個層次時，往往也令人頭痛。沒有經驗的開發者很難直觀的感受到計算圖到底在幹些什麼。儘管很讓人頭痛，但大多數的深度學習框架使用的都是符號計算這一套方法，因為符號計算能夠提供關鍵的計算優化、自動求導等功能。

我們建議你在使用前稍微了解一下Theano或TensorFlow，Bing/Google一下即可。

## 張量

張量，或tensor，是本文檔會經常出現的一個詞彙，在此稍作解釋。

使用這個詞彙的目的是為了表述統一，張量可以看作是向量、矩陣的自然推廣，我們用張量來表示廣泛的數據類型。

規模最小的張量是0階張量，即標量，也就是一個數。

當我們把一些數有序的排列起來，就形成了1階張量，也就是一個向量

如果我們繼續把一組向量有序的排列起來，就形成了2階張量，也就是一個矩陣

把矩陣摞起來，就是3階張量，我們可以稱為一個立方體，具有3個顏色通道的彩色圖片就是一個這樣的立方體

把立方體摞起來，好吧這次我們真的沒有給它起別名了，就叫4階張量了，不要去試圖想像4階張量是什麼樣子，它就是個數學上的概念。

張量的階數有時候也稱為維度，或者軸，軸這個詞翻譯自英文axis。譬如一個矩陣[[1,2],[3,4]]，是一個2階張量，有兩個維度或軸，沿著第0個軸（為了與python的計數方式一致，本文檔維度和軸從0算起）你看到的是[1,2]，[3,4]兩個向量，沿著第1個軸你看到的是[1,3]，[2,4]兩個向量。

要理解“沿著某個軸”是什麼意思，不妨試著運行一下下面的代碼：

```python
import numpy as np

a = np.array([[1,2],[3,4]])
sum0 = np.sum(a, axis=0)
sum1 = np.sum(a, axis=1)

print sum0
print sum1
```

關於張量，目前知道這麼多就足夠了。事實上我也就知道這麼多

## data_format

這是一個無可奈何的問題，在如何表示一組彩色圖片的問題上，Theano和TensorFlow發生了分歧，'th'模式，也即Theano模式會把100張RGB三通道的16×32（高為16寬為32）彩色圖表示為下面這種形式（100,3,16,32），Caffe採取的也是這種方式。第0個維度是樣本維，代表樣本的數目，第1個維度是通道維，代表顏色通道數。後面兩個就是高和寬了。這種theano風格的數據組織方法，稱為“channels_first”，即通道維靠前。

而TensorFlow，的表達形式是（100,16,32,3），即把通道維放在了最後，這種數據組織方式稱為“channels_last”。

Keras默認的數據組織形式在~/.keras/keras.json中規定，可查看該文件的`image_data_format`一項查看，也可在代碼中通過K.image_data_format()函數返回，請在網絡的訓練和測試中保持維度順序一致。

唉，真是蛋疼，你們商量好不行嗎？



<a name='functional'>
<font color='#404040'>
## 函數式模型
</font>
</a>

函數式模型算是本文檔比較原創的詞彙了，所以這裡要說一下

在Keras 0.x中，模型其實有兩種，一種叫Sequential，稱為序貫模型，也就是單輸入單輸出，一條路通到底，層與層之間只有相鄰關係，跨層連接統統沒有。這種模型編譯速度快，操作上也比較簡單。第二種模型稱為Graph，即圖模型，這個模型支持多輸入多輸出，層與層之間想怎麼連怎麼連，但是編譯速度慢。可以看到，Sequential其實是Graph的一個特殊情況。

在Keras1和Keras2中，圖模型被移除，而增加了了“functional model API”，這個東西，更加強調了Sequential是特殊情況這一點。一般的模型就稱為Model，然後如果你要用簡單的Sequential，OK，那還有一個快捷方式Sequential。

由於functional model API在使用時利用的是“函數式編程”的風格，我們這裡將其譯為函數式模型。總而言之，只要這個東西接收一個或一些張量作為輸入，然後輸出的也是一個或一些張量，那不管它是什麼鬼，統統都稱作“模型”。

<a name='batch'>
<font color='#404040'>
## batch
</font></a>

這個概念與Keras無關，老實講不應該出現在這裡的，但是因為它頻繁出現，而且不了解這個技術的話看函數說明會很頭痛，這裡還是簡單說一下。

深度學習的優化算法，說白了就是梯度下降。每次的參數更新有兩種方式。

第一種，遍歷全部數據集算一次損失函數，然後算函數對各個參數的梯度，更新梯度。這種方法每更新一次參數都要把數據集裡的所有樣本都看一遍，計算量開銷大，計算速度慢，不支持在線學習，這稱為Batch gradient descent，批梯度下降。

另一種，每看一個數據就算一下損失函數，然後求梯度更新參數，這個稱為隨機梯度下降，stochastic gradient descent。這個方法速度比較快，但是收斂性能不太好，可能在最優點附近晃來晃去，hit不到最優點。兩次參數的更新也有可能互相抵消掉，造成目標函數震蕩的比較劇烈。

為了克服兩種方法的缺點，現在一般採用的是一種折中手段，mini-batch gradient decent，小批的梯度下降，這種方法把數據分為若干個批，按批來更新參數，這樣，一個批中的一組數據共同決定了本次梯度的方向，下降起來就不容易跑偏，減少了隨機性。另一方面因為批的樣本數與整個數據集相比小了很多，計算量也不是很大。

基本上現在的梯度下降都是基於mini-batch的，所以Keras的模塊中經常會出現batch_size，就是指這個。

順便說一句，Keras中用的優化器SGD是stochastic gradient descent的縮寫，但不代表是一個樣本就更新一回，還是基於mini-batch的。


## epochs

真的不是很想解釋這個詞，但是新手問的還挺多的……
簡單說，epochs指的就是訓練過程中數據將被“輪”多少次，就這樣。

## 對新手友好的小說明

雖然這不是我們應該做的工作，但為了體現本教程對新手的友好，我們在這裡簡單列一下使用keras需要的先行知識。稍有經驗的研究者或開發者請忽略本節，對於新手，我們建議在開始之前，確保你了解下面提到的術語的基本概念。如果你確實對某項內容不了解，請首先查閱相關資料，以免在未來使用中帶來困惑。

### 關於Python

* 顯然你應對Python有一定的熟悉，包括其基本語法，數據類型，語言特點等，如果你還不能使用Python進行程序設計，或不能避免Python中常見的一些小陷阱，或許你應該先去找個教程補充一下。這裡推一個快速學習Python的教程[<font color='#FF0000'>廖雪峰的Python教程</font>](http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000)

* 你應該有面向對象的概念，知道類、對象、封裝、多態、繼承、作用域等術語的含義。

* 你應該對Python的科學計算包和深度學習包有一定了解，這些包包含但不限於numpy, scipy, scikit-learn, pandas...

* 特別地，你需要了解什麼是生成器函數（generator），以及如何編寫生成器函數。什麼是匿名函數（lambda）

### 關於深度學習

由於Keras是為深度學習設計的工具，我們這裡只列舉深度學習中的一些基本概念。請確保你對下面的概念有一定理解。

* 有監督學習，無監督學習，分類，聚類，回歸

* 神經元模型，多層感知器，BP算法

* 目標函數（損失函數），激活函數，梯度下降法

* 全連接網絡、卷積神經網絡、遞歸神經網絡

* 訓練集，測試集，交叉驗證，欠擬合，過擬合

* 數據規範化

* 其他我還沒想到的東西……想到再補充


## 其他

其他需要注意的概念，我們將使用[Tips]標註出來，如果該概念反復出現又比較重要，我們會寫到這裡。就醬，玩的愉快喲。